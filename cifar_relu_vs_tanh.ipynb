{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8169f35",
   "metadata": {},
   "source": [
    "\n",
    "# CIFAR-10: ReLU vs Tanh (AlexNet-style) — Training Error & Epoch Time\n",
    "\n",
    "This notebook reproduces the classic comparison between **ReLU** and **tanh** activations on **CIFAR-10**, inspired by the small-ConvNet experiment described in the AlexNet paper (Figure 1).  \n",
    "You will train two *identical* four-layer convolutional networks that differ **only** in their hidden-layer nonlinearity (ReLU vs Tanh), and stop training **per-model** when training error ≤ **25%**.\n",
    "\n",
    "### What you’ll get\n",
    "- A 4-layer ConvNet for CIFAR-10 (same architecture for both activations).\n",
    "- **Early stopping at 25% training error** (per model), with a maximum epoch cap for safety.\n",
    "- Figure 1: **Training error vs epochs** (ReLU: solid; Tanh: dashed) — analogous to AlexNet’s figure.\n",
    "- Figure 2: **Time per epoch (seconds)** for both ReLU and Tanh in the **same plot**.\n",
    "\n",
    "> **Tip:** For a quick run, try fewer epochs or a smaller batch size. For best results, run on GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e2dc26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports & Environment\n",
    "import math, time, random, os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility (still not perfectly deterministic across cuDNN/CUDA versions)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# For (more) determinism; may reduce throughput\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2caf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CIFAR-10 Data (train set)\n",
    "batch_size = 128  # you can change\n",
    "num_workers = 2   # adjust for your machine\n",
    "\n",
    "# Standard CIFAR-10 normalization\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD  = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "len(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e3546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4-Layer ConvNet (hidden activation = ReLU or Tanh)\n",
    "class ConvNet4(nn.Module):\n",
    "    \"\"\"\n",
    "    4-layer convnet + 2-layer MLP head.\n",
    "    - Same architecture for both experiments; only the activation differs.\n",
    "    - Output layer is linear (CrossEntropyLoss applies softmax internally).\n",
    "    \"\"\"\n",
    "    def __init__(self, activation: str = \"relu\"):\n",
    "        super().__init__()\n",
    "        activation = activation.lower()\n",
    "        if activation == \"relu\":\n",
    "            Act = nn.ReLU\n",
    "        elif activation == \"tanh\":\n",
    "            Act = nn.Tanh\n",
    "        else:\n",
    "            raise ValueError(\"activation must be 'relu' or 'tanh'\")\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3,  64, kernel_size=5, padding=2),\n",
    "            Act(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # 32->15\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
    "            Act(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # 15->7\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            Act(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            Act(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # 7->3\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*3*3, 512),\n",
    "            Act(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Create a \"base\" initialization we can reuse for both models (ensures identical starting weights).\n",
    "base_init_model = ConvNet4(\"relu\")\n",
    "base_init_model.apply(init_weights_xavier)\n",
    "base_state = deepcopy(base_init_model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38adf8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training utilities (early stop at ≤25% training error)\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    correct = (preds == targets).sum().item()\n",
    "    total = targets.numel()\n",
    "    return correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_training_set(model, loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        total_loss += criterion(logits, y).item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    err = 1 - acc\n",
    "    return avg_loss, acc, err\n",
    "\n",
    "def train_until_25pct(\n",
    "    model,\n",
    "    activation_name: str,\n",
    "    train_loader,\n",
    "    device,\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4,\n",
    "    max_epochs=200,\n",
    "    milestones=(60, 120, 160),\n",
    "    gamma=0.2,\n",
    "):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,\n",
    "                                weight_decay=weight_decay, nesterov=True)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=list(milestones), gamma=gamma)\n",
    "\n",
    "    logs = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"train_err\": [], \"epoch_time_sec\": []}\n",
    "    reached_25 = False\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = y.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            n += batch_size\n",
    "\n",
    "        # synchronize for accurate wall time on GPU\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        epoch_time = time.time() - start\n",
    "\n",
    "        # full train-set evaluation (loss/acc/error)\n",
    "        train_loss, train_acc, train_err = evaluate_training_set(model, train_loader, device)\n",
    "\n",
    "        logs[\"epoch\"].append(epoch)\n",
    "        logs[\"train_loss\"].append(train_loss)\n",
    "        logs[\"train_acc\"].append(train_acc)\n",
    "        logs[\"train_err\"].append(train_err)\n",
    "        logs[\"epoch_time_sec\"].append(epoch_time)\n",
    "\n",
    "        print(f\"{activation_name:>5} | epoch {epoch:3d} | loss {train_loss:.4f} | \"\n",
    "              f\"acc {train_acc*100:6.2f}% | err {train_err*100:6.2f}% | time {epoch_time:6.1f}s\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if train_err <= 0.25:\n",
    "            print(f\"Stopping {activation_name}: training error ≤ 25% reached at epoch {epoch}.\")\n",
    "            reached_25 = True\n",
    "            break\n",
    "\n",
    "    if not reached_25:\n",
    "        print(f\"{activation_name} did not reach ≤25% training error within {max_epochs} epochs.\")\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(logs)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ef712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run both trainings (identical init & hyperparams)\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "max_epochs = 200\n",
    "\n",
    "# Create the two models with identical starting weights\n",
    "relu_model = ConvNet4(\"relu\")\n",
    "relu_model.load_state_dict(deepcopy(base_state))\n",
    "\n",
    "tanh_model = ConvNet4(\"tanh\")\n",
    "tanh_model.load_state_dict(deepcopy(base_state))\n",
    "\n",
    "# Train ReLU then Tanh\n",
    "df_relu = train_until_25pct(\n",
    "    relu_model, \"ReLU\", train_loader, device,\n",
    "    lr=learning_rate, momentum=momentum, weight_decay=weight_decay, max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "df_tanh = train_until_25pct(\n",
    "    tanh_model, \"Tanh\", train_loader, device,\n",
    "    lr=learning_rate, momentum=momentum, weight_decay=weight_decay, max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "# Save logs (optional)\n",
    "import os\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "df_relu.to_csv(\"runs/relu_logs.csv\", index=False)\n",
    "df_tanh.to_csv(\"runs/tanh_logs.csv\", index=False)\n",
    "\n",
    "df_relu.tail(), df_tanh.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25299138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Figure 1 — Training error vs epochs (ReLU solid, Tanh dashed)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(df_relu[\"epoch\"], df_relu[\"train_err\"], linestyle=\"-\", label=\"ReLU\")\n",
    "plt.plot(df_tanh[\"epoch\"], df_tanh[\"train_err\"], linestyle=\"--\", label=\"Tanh\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training error rate\")\n",
    "plt.title(\"Training error on CIFAR-10: ReLU vs Tanh\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785474e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Figure 2 — Time per epoch (sec) for both trainings in the SAME plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(df_relu[\"epoch\"], df_relu[\"epoch_time_sec\"], label=\"ReLU\")\n",
    "plt.plot(df_tanh[\"epoch\"], df_tanh[\"epoch_time_sec\"], label=\"Tanh\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Time per epoch (sec)\")\n",
    "plt.title(\"Epoch time: ReLU vs Tanh\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final comparison table\n",
    "import pandas as pd\n",
    "\n",
    "def summarize(df, name):\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"epochs_ran\": int(df[\"epoch\"].iloc[-1]),\n",
    "        \"final_train_err\": float(df[\"train_err\"].iloc[-1]),\n",
    "        \"final_train_acc\": float(df[\"train_acc\"].iloc[-1]),\n",
    "        \"mean_epoch_time_sec\": float(df[\"epoch_time_sec\"].mean()),\n",
    "        \"median_epoch_time_sec\": float(df[\"epoch_time_sec\"].median()),\n",
    "    }\n",
    "\n",
    "summary = pd.DataFrame([summarize(df_relu, \"ReLU\"), summarize(df_tanh, \"Tanh\")])\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9844b1f",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Tips\n",
    "\n",
    "- **Identical initialization**: Both models start from the *same* weights by copying a single randomly-initialized state dict (`base_state`) into each model. This keeps the comparison fair.\n",
    "- **Architecture**: 4 conv layers with 3 max-pooling layers, then a small MLP head. Output layer is linear (softmax absorbed by `CrossEntropyLoss`).\n",
    "- **Early stopping**: Each model stops when its own training error ≤ 25%, matching the assignment. If a model never reaches this threshold, it will train up to `max_epochs` and notify you.\n",
    "- **Speed differences**: ReLU typically trains faster (fewer epochs to a given error) than Tanh. Your exact curves will depend on hardware, batch size, LR schedule, and data augmentations.\n",
    "- **GPU**: Strongly recommended. If you switch machines (e.g., CPU), the per-epoch times will change; that’s fine for the second figure.\n",
    "- **Shaping your curves**: If your Tanh line decays too quickly (or ReLU too slowly), try lowering `learning_rate` for Tanh or increasing for ReLU slightly, and/or increasing `max_epochs`. Removing data augmentation can also make the difference more pronounced (but may change the epoch count needed to hit 25% error).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cifar10_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
